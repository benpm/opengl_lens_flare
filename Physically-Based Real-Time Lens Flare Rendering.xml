<?xml version="1.0" encoding="UTF-8" ?>
<!-- Created from PDF via Acrobat SaveAsXML -->
<!-- Mapping Table version: 28-February-2003 -->
<TaggedPDF-doc>
<?xpacket begin='﻿' id='W5M0MpCehiHzreSzNTczkc9d'?>
<?xpacket begin="﻿" id="W5M0MpCehiHzreSzNTczkc9d"?>
<x:xmpmeta xmlns:x="adobe:ns:meta/" x:xmptk="Adobe XMP Core 9.1-c001 79.675d0f7, 2023/06/11-19:21:16        ">
   <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
      <rdf:Description rdf:about=""
            xmlns:dc="http://purl.org/dc/elements/1.1/"
            xmlns:xmp="http://ns.adobe.com/xap/1.0/"
            xmlns:pdf="http://ns.adobe.com/pdf/1.3/"
            xmlns:pdfx="http://ns.adobe.com/pdfx/1.3/"
            xmlns:xmpMM="http://ns.adobe.com/xap/1.0/mm/">
         <dc:format>xml</dc:format>
         <dc:creator>
            <rdf:Seq>
               <rdf:li>Matthias B. Hullin, Elmar Eisemann, Hans-Peter Seidel, Sungkil Lee</rdf:li>
            </rdf:Seq>
         </dc:creator>
         <dc:description>
            <rdf:Alt>
               <rdf:li xml:lang="x-default"/>
            </rdf:Alt>
         </dc:description>
         <dc:title>
            <rdf:Alt>
               <rdf:li xml:lang="x-default">Physically-Based Real-Time Lens Flare Rendering</rdf:li>
            </rdf:Alt>
         </dc:title>
         <xmp:CreateDate>2011-05-04T12:30:48+02:00</xmp:CreateDate>
         <xmp:CreatorTool>LaTeX acmsiggraph.cls</xmp:CreatorTool>
         <xmp:ModifyDate>2025-08-24T19:45:11-06:00</xmp:ModifyDate>
         <xmp:MetadataDate>2025-08-24T19:45:11-06:00</xmp:MetadataDate>
         <pdf:Keywords>Lens flare, Real-time rendering</pdf:Keywords>
         <pdf:Producer>pdfTeX-1.40.3</pdf:Producer>
         <pdf:Trapped>False</pdf:Trapped>
         <pdfx:PTEX.Fullbanner>This is pdfTeX using libpoppler, Version 3.141592-1.40.3-2.2 (Web2C 7.5.6) kpathsea version 3.5.6</pdfx:PTEX.Fullbanner>
         <xmpMM:DocumentID>uuid:e125550e-ed3e-4098-b6f1-4a78461469d4</xmpMM:DocumentID>
         <xmpMM:InstanceID>uuid:905f72ae-82bf-4b83-8887-416739034e0d</xmpMM:InstanceID>
      </rdf:Description>
   </rdf:RDF>
</x:xmpmeta>
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                           
<?xpacket end="w"?>
<?xpacket end='r'?>

<Part>
<H3>Physically-Based Real-Time Lens Flare Rendering </H3>

<P>Matthias Hullin1 Elmar Eisemann2,3 Hans-Peter Seidel1,3 Sungkil Lee4,1 </P>

<P>1 MPI Informatik 2 Tel´ ´ 3 Saarland University </P>

<P>ecom ParisTech 4 Sungkyunkwan University </P>
<Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_0.jpg"/>
</Figure>

<P>Figure 1: Complex lens fare generated by a Canon zoom lens. Left: reference photos. Right: renderings generated using our technique at comparable settings. Even with many unknowns in the lens design and scene composition, as well as manufacturing tolerances in the real lens, the renderings closely reproduce the “personality” of the fare. </P>

<Sect>
<Sect>
<H5>Abstract </H5>

<P>Lens fare is caused by light passing through a photographic lens system in an unintended way. Often considered a degrading artifact, it has become a crucial component for realistic imagery and an artistic means that can even lead to an increased perceived brightness. So far, only costly offine processes allowed for convincing simulations of the complex light interactions. In this paper, we present a novel method to interactively compute physically-plausible fare renderings for photographic lenses. The underlying model covers many components that are important for realism, such as imperfections, chromatic and geometric lens aberrations, and antirefective lens coatings. Various acceleration strategies allow for a performance/quality tradeoff, making our technique applicable both in real-time applications and in high-quality production rendering. We further outline artistic extensions to our system. </P>

<P>CR Categories: I.3.3 [Computer Graphics]: Image Generation </P>

<P>Keywords: Lens fare, Real-time rendering </P>

<P>Contact to authors: {hullin </P>

<P>hpseidel}@mpi-inf.mpg.de elmar.eisemann@telecom-paristech.fr sungkil@skku.edu (Corresponding author) </P>

<Sect>
<H5>1 Introduction </H5>

<P>Lens fare is an effect caused by light passing through a photographic lens in any other way than the one intended by design—most importantly through interrefection between optical elements (ghosting). Flare becomes most prominent when a small number of very bright lights is present in a scene. In traditional photography and cinematography, lens fare is considered a degrading artifact and therefore undesired. Among the measures to reduce stray light in an optical system are optimized barrel designs, anti-refective coatings, and lens hoods. </P>

<P>On the other hand, fare-like effects are often used deliberately to suggest the presence of very bright light sources, hence increasing the perceived realism. In fact, nowadays the use of lens fare is every bit as popular in games as it is in image and video editing. For the production of computer-generated movies, great effort has been taken to model cinema lenses with all their physical faws and 
<Link>limitations [Pixar 2008]. </Link>
</P>

<P>The problem of rendering lens fare has been approached from two ends. A very simple and effcient, but not quite accurate, technique is the use of static textures (starbursts, circles, and rings) that move according to the position of the light source, and are composited additively to the base image. Flares generated from texture billboards can look convincing in many situations, yet they fail to capture the intricate dynamics and variations of real lens fare. </P>

<P>At the other end of the scale, sophisticated techniques have been demonstrated that involve ray or path tracing through a virtual lens with all of its optical elements. The results are near-accurate but very costly to compute, with typical rendering times in the order of several hours per frame on a current desktop computer. Furthermore, many samples end up being blocked in the lens system, which wastes much of the computation time and leads to slow convergence. Also, the solution only holds within the limits of geometric optics. Some phenomena encountered in real lens fares, however, are caused by wave-optical effects. Integrating them into a ray-optical framework is by no means trivial and further increases the computational cost. </P>

<P>By combining sparse ray tracing and rasterization, our rendering technique can simulate lens fares of complex lens designs, achieving a high degree of realism at interactive frame rates. Chromatic and geometric lens aberrations are reproduced naturally and can be extended by more advanced effects. </P>

<P>Precisely, we make the following contributions: </P>

<L>
<LI>
<Lbl>• </Lbl>

<LBody>a model for realistic lens fare; </LBody>
</LI>

<LI>
<Lbl>• </Lbl>

<LBody>an effcient algorithm that allows a fne-tuned tradeoff between quality and effciency; </LBody>
</LI>

<LI>
<Lbl>• </Lbl>

<LBody>plausible approximations for diffcult-to-handle imperfections. </LBody>
</LI>
</L>

<P>The latter also represents a means for stylization. We show that expressive lens fare is a natural extension of our work. </P>
</Sect>

<Sect>
<H5>2 Previous Work </H5>

<P>Computer graphics research often focuses on the simulation of light exchange and interaction inside a virtual environment. While such computations can deliver physically-plausible imagery, a certain lack of realism remains where simplifed camera models fall short of their real counterparts. Many effects (e.g., depth of feld or motion blur) are crucial components for realistic image synthesis and many researchers underlined the need to focus more closely on camera 
<Link>specifcities [Kolb et al. 1995; Lee et al. 2010; Steinert et al. 2011]. </Link>
</P>

<P>More faithful camera or lens-system simulation also covers stray light which adds a signifcant amount of realism to the rendering. It can in part be caused by dust or imperfections in the lens system, but the most prominent features of lens fare originate from internal 
<Link>refections (ghosting) [Kingslake 1992]. </Link>
</P>

<P>What makes the simulation of lens fare attractive is the observation that humans are trained to interpret the presence of fare and veiling glare as an indication of extreme brightness. This perceptual effect can be used to seemingly exceed the physical boundaries of a dis
<Link>play device [Ritschel et al. 2009], leading to its strong </Link>
use in, e.g., 
<Link>movies [Pixar 2008], or recent games </Link>

<Link>[Wenzel 2005]. </Link>
</P>

<P>Previous interactive techniques relied on signifcant approximations. 
<Link>Kilgard [2000] suggested the </Link>
use of texture sprites that are blended into the framebuffer and arranged on a line through the screen center, following an ad hoc displacement function. 
<Link>King [2001] </Link>
varied sprite size and opacity depending on the angle between light source and camera. 
<Link>Maughan [2001] added a </Link>
brightness variation that can also be controlled depending on the number of visible pixels of an area 
<Link>light [Sekulic 2004]. Oat [2004] concentrated </Link>
on light streaks that are added using a steerable flter. 
<Link>[Alspach 2009] described lens </Link>
fare as a set of vector shapes such as “halo”, “rays” or “rings” that are generated according to user-specifed statistics. In none of these cases, an underlying camera or lens model was considered. </P>

<P>In other situations, more accurate simulations are needed, e.g., when compositing virtual and realistic content, when designing lens systems, or when predicting the appearance of a scene through a lens system. 
<Link>Previous high-quality approximations [Chaumond 2007; </Link>

<Link>Keshmirian 2008] relied </Link>
on path tracing or photon mapping. While such approaches deliver theoretically a high quality, several aspects; such as spectral (e.g., chromatic aberration or lens coating), diffraction effects, or aperture shape, are usually ignored. Furthermore, the visual quality for short computation times can be insuffcient, making interaction (e.g., zooming) impossible. </P>

<P>Simulation of wave-optical effects (in particular, diffraction and interference) is usually considered out of reach for graphics applications. High-end optical design tools such as ZEMAX or Code V allow for the computation of point spread functions and even coarse 
<Link>predictions of stray light including diffraction effects [Tocci </Link>

<Link>2007
<Link>; Perrin 2004]. Notwithstanding their physical accuracy, they </Link>
</Link>
offer very general solutions and hence are not optimized for effcient 
<Link>high-quality fare rendering. In a purely ray-based framework, [Oh </Link>

<Link>et al. 2010] showed how </Link>
diffraction-like effects can be emulated for simple regular structures for which the light feld transforms can be expressed analytically. In this work, we approximate similar effects for general aperture shapes as a preprocessing step, using Fourier transforms. </P>

<P>Our rendering scheme is physically motivated, yet runs at interactive to real-time performance. Based on a ray-tracing approach, the technique does not only consider individual rays, but exploit the strong correlation of rays within a light bundle. Radiant fux is treated in a way similar to beam or pencil tracing (for a comprehensive 
<Link>literature overview, see e.g. [Ernst et al. 2005]), but without the need </Link>
for adaptive refnement. </P>

<P>Further, our solution can be adapted to exaggerate or replace physical components. Its initial faithfulness ensures that the resulting imagery keeps a convincing and plausible appearance even after applying signifcant artistic tweaks. </P>
</Sect>
</Sect>

<Sect>
<Sect>
<H5>3 Model of the Optical System </H5>
<Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_1.jpg"/>
Description of optical systemLens designAR coatingsApertureMaterialsLight transport modelDielectric reflec-tion/transmissionScaling paramsBounce ordersDiffraction</Figure>

<P>Figure 2: Building bricks of our model. Each element allows to trade physical realism against artistic expression, or detail against performance. </P>

<P>In this section, we discuss various aspects of an optical system and describe how we mathematically represent them. Depending on the requirements of the application, some effects can be skipped to simplify the model and increase the performance. The following should be considered building bricks that can either be modeled as accurately as desired, exaggerated, or altered in an artistically desired way. 
<Link>Figure 2 illustrates the components of our system and </Link>
gives an overview over our model of the optical system, as well as 
<Link>the light transport, and implementation (Section 4). </Link>
</P>

<P>Geometry Light propagation is governed by light transmission through, and refection at a set of lens surfaces and characteristic planes (entrance, aperturer, and sensor plane). In our examples, we follow 
<Link>the defnitions of photographic lenses from [Smith 2005], </Link>
as well as the patent describing a lens we have 
<Link>at hand [Ogawa 1996]. </Link>
The geometric model is realized as a set of algebraically defned surfaces, i.e., spheres and planes. Note that our technique does not impose particular geometry or lens materials. Further, rules of good optical design are not always needed to achieve attractive fare. </P>
<Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_2.jpg"/>
</Figure>

<P>Iris Aperture The aperture (also called diaphragm, iris, or stop) consists of mechanical blades that control the size of a pupil by rotating into place. </P>

<P>When the aperture is fully open, </P>

<P>Figure 3: Single blade (left) </P>

<P>they are hidden in the lens bar-</P>

<P>and a polygonal iris (right) </P>

<P>rel, resulting in a circular cross-</P>

<P>section. “Stopping down” the aperture leads to a polygonal contour defned by number, shape, and position of the blades. We 
<Link>recreate this mechanism (Figure 3) </Link>
and store the resulting mask in a texture. </P>

<P>Optical Media and Dispersion In terms of optical media, we constrain ourselves to perfect dielectrics with a real-valued refractive index. All optical glasses are dispersive media, i.e., the refractive index n is a function of the wavelength of light, λ. We follow Sellmeier’s 
<Link>empirical approximation [Sellmeier 1871] to describe </Link>
the dispersion of optical glasses: </P>

<P>2 B1λ2 B2λ2 B3λ2 </P>

<P>n (λ)=1+ + + , (1)</P>

<P>λ2 − C1 λ2 − C2 λ2 − C3 </P>

<P>where B{1,2,3} and C{1,2,3} are material constants that can be ob
<Link>tained from manufacturer databases, e.g. [Schott AG 2011], </Link>
or other sources 
<Link>[Polyanskiy 2010]. </Link>
</P>
</Sect>

<Sect>
<Sect>
<H5>3.1 Refection and Transmission: Fresnel Equations </H5>

<P>Every time a ray of light hits an interface between two media, a part of it is refected, and the rest transmitted. It is the refected part that gives rise to ghosting artifacts, which we seek to simulate. For smooth surfaces, the relative amounts follow Fresnel’s equations, with the resulting ray directions according to the law of refection and Snell’s 
<Link>law, respectively [Hecht 2001]. </Link>
</P>

<P>The Fresnel equations provide different transmission and refection coeffcients for different states of polarization. For unpolarized light propagating from medium 1 to medium 2 (with refractive indices ni and angles with respect to the normal θi), the overall refectivity R and transmissivity T of a surface can be expressed as </P>

<P>„«2 „«2</P>

<P>1 n1 cos θ1 − n2 cos θ2 1 n1 cos θ2 − n2 cos θ1</P>

<P>R =+ </P>

<P>2 n1 cos θ1 + n2 cos θ2 2 n1 cos θ2 + n2 cos θ1 </P>

<P>and T =1 − R. </P>

<P>The treatment of polarization is crucial for the inner workings of an anti-refective coating (see below). Since Fresnel refection or transmission are partially polarizing, sequences of light-surface interaction would ideally keep track of a light ray’s polarization state. However, since the marginal beneft would not justify the expenses, our current implementation of the ray tracer does not propagate polarization throughout the optical system. At each optical surface, the incident light is assumed to be s-and 
<Link>p-polarized (see [Hecht 2001] for </Link>
a defnition) to equal parts. </P>
<Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_3.jpg"/>
  0510152025303540454004505005506006507001.822.22.42.62.833.2degnm</Figure>

<P>Figure 4: Lens coatings make refections from optical interfaces inside the lens barrel appear colored. Left: Canon EF 70-200mm f/2.8L. Middle: Canon EF 100mm f/2.8 USM macro. Right: Net refectivity in % of a quarter-wave coating designed for 532 nm light at normal incidence (nglass =1.5, ncoating =1.38, d = 96.4 nm). </P>

<P>Anti-Refection Coatings In an attempt to minimize refections, optical surfaces often feature antirefective coatings. They consist of layers of clear materials with different refractive index. Light waves that are refected at different interfaces are superimposed and interfere with each other. In particular, if two refections have opposite phase and identical amplitude, they cancel each other out, reducing the net refectivity of the surface. The parameters of the multi-layer coatings used for high-end lenses are well-kept secrets of the manufacturers. </P>

<P>Even the best available coatings are not perfect. Their residual refectivity is a function of wavelength and angle, R(λ, θ). A look into a real lens reveals that different interfaces refect white light in different colors, suggesting that they are all coated differently 
<Link>(Figure 4). </Link>
</P>
</Sect>

<P>Without the resources to reverse-engineer exact characteristics, we chose a so-called “quarter-wave” coating. It consists of a single thin layer. With such coating, the refectivity of the surface can be minimized for a center wavelength λ0 for a given angle of incidence, θ0. This requires a solid material of very low refractive index; in practice, the best choice is often MgF2 (n =1.38). The layer thickness is chosen to result in a phase shift of π/2 (hence the name). </P>

<P>While an analytical expression for R(λ, θ) can be derived in most cases, even the simple quarter-wave coating involves multiple instances of the Fresnel equations, making the expression relatively complex. An example plot for a quarter-wave coating is shown in 
<Link>Figure 4. </Link>
One way to approximate such a function is to store it in a precomputed 2D texture, which enables us to also record or use arbitrary available coating functions. Nonetheless, in practice, the GPU’s arithmetic power is usually high enough to evaluate the function directly. The computation scheme for R(λ, θ) as used in our shader is provided as supplemental material. </P>
</Sect>

<Sect>
<Sect>
<H5>3.2 Absorptance </H5>

<P>All optical glasses partially absorb light that passes through them. However, this is a weak effect (with typical light loss of a few percent across the entire lens system) and of low frequency (global attenuation). We therefore chose not to include it in our model. </P>
</Sect>
</Sect>

<Sect>
<Sect>
<H5>3.3 Diffraction </H5>
</Sect>

<P>When light passes through small-scale geometry such as the iris aperture in our system, or small imperfections (fngerprints, dust, scratches), it is diffracted into geometrically shadowed regions. The physical explanation can be found in Huygens’ principle which states that every point on a wavefront can be thought of as the emitter of a spherical wave. The resulting pattern is defned by the superposition 
<Link>of these elementary waves [Hecht 2001; Goodman 2005]. </Link>
</P>

<Sect>
<P>In our system, we encounter two typical occurrences of diffraction: the starburst shape centered around the image of the light source and the subtle ringing patterns around the border of each refection ghost. In a wave-optical framework, both can be computed exactly by evaluating the so-called diffraction integral for all points on the sensor. This is very costly and by no means possible in real time. Instead, we are interested in a computationally cheap approximation. As it turns out, we can convincingly reproduce the above effects by precomputing a small set of textures using the popular Fraunhofer and Fresnel approximations to the diffraction integral, respectively. </P>

<P>Starburst pattern – Fraunhofer approximation Light passing through a transmissive aperture and propagating further through free space is diffracted into a characteristic far-feld pattern. Assuming a uniform incident distribution of parallel (collimated) light (at wavelength λ) and a real-valued amplitude transmission function T (x, y), the Fraunhofer pattern T 0(x 0 ,y 0) in an observation plane at distance z0 from the aperture is given by the Fourier power spectrum (i.e., the squared-magnitude Fourier spectrum) of T 
<Link>[Hecht 2001]. The </Link>
relation between image coordinates (x 0 ,y 0) and Fourier frequencies (u, v) is given by: </P>
</Sect>
<Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_4.jpg"/>
</Figure>

<Sect>
<P>Figure 5: Left: Photo of a bright light source. Right: Chromatic Fourier transform of an aperture transmission function. </P>

<P>(x 0 ,y 0)=(u, v) · λ · z0 (2) </P>

<P>For a given aperture geometry, it is therefore suffcient to compute a single Fourier spectrum and scale it linearly depending on the wavelength. We start with the polygonal transmission function in unit size and optionally add some 
<Link>noise (Figure 14, right). Similar </Link>
to [Ritschel2009], we then compute a 3-channel starburst texture VRGB(s, t) by superimposing multiple scaled copies of the power spectrum of the aperture. We found wavelength steps of 5 nm to be suffciently fne to blur out the radial ringing present in the individual spectral terms. 
<Link>The fnal texture (see Figure 5 for </Link>
an example) is normalized to unit radiant fux per color channel. During runtime, we center it at the projected sensor location of the light source, and scale it in size (w, h) and intensity I as follows: </P>

<P>w = h = w0 · # // scale with reciprocal aperture size (3) </P>

<P>I = I0 · IRGB · #−4 // #−2 of light transmitted thru iris (4) </P>

<P>// spread over #2 the starburst area </P>

<P>where # is the f-number (as in “f/#”), and IRGB the radiant fux entering the lens expressed as an RGB vector. We are not aware of a simple way of obtaining the accurate scaling constants w0 (“size”) and I0 (“intensity”) that are specifc to the optical system. Instead, since both parameters are rather intuitive, we leave this choice to the user, allowing them to either recreate the appearance of a given optical system, or to amplify and resize the starburst as they desire. </P>

<P>Ringing pattern – Fresnel approximation Optical systems are usually laid out such that the aperture plane is Fourier transformed into the sensor plane by the back section of the lens system. In the presence of interrefections, arbitrary transforms are possible, ranging from an image of the aperture itself to its Fourier transform, but in particular intermediate patterns that share features with both the spatial and Fourier domains. </P>

<P>Mathematically, the continuum between the spatial and Fourier domains can be expressed in terms of fractional powers of the Fourier operator. Research in wave optics shows a close relation between this Fractional Fourier Transform (FrFT) and the so-called Fresnel approximation for 
<Link>near-feld diffraction [Ozaktas et al. 2001]. With</Link>
out the need for a deeper understanding of Fourier optics, we can use 
<Link>the FrFT code from [Ozaktas et al. 2001] </Link>
to embellish given aperture functions with plausible ringing patterns (Figure 6). It does not take much effort to fnd a good value for the only parameter, the fractional order α, to reproduce a given fare pattern. Heuristically, we found the following to work well (longer wavelength and smaller aperture leads to stronger ringing): </P>

<P>α =0.15 · (λ/400 nm) · (#/18) (5) </P>

<P>As we will see in the next section, these precomputed textures can be used during our rasterization step, where they replace the sharp aperture image that would result from standard ray tracing. </P>
</Sect>

<P>Figure 6: Left: Close-up on a lens fare photo. Note the ringing around the contour of each ghost. Middle: Chromatic FrFT of an octagonal aperture. Right: Gradual transition from spatial to Fourier domain (0 ≤α≤1 in steps of 0.2). </P>
</Sect>
</Sect>

<Sect>
<Sect>
<H5>4 Rendering System </H5>

<P>The previous section analyzed how to model important aspects of lens-fare effects. Here, we present our rendering technique to simulate the actual light propagation. It is based on ray tracing through the optical system to the flm plane (sensor). In contrast to expensive 
<Link>
<Link>off-line approaches [Keshmirian 2008; Steinert et al</Link>
. 2011], we only </Link>
trace a sparse set of rays. Each ray records values about the lens-system traversal. Rays reaching the sensor implicitly defne a ray grid across which we then interpolate the recorded values in image space. Hereby, we can approximate the outcome of rays that were never actually shot, leading to an approximate beam tracing. The overview of 
<Link>our rendering pipeline is illustrated in Figure 7. In the </Link>
following, we will elaborate on the stages of our pipeline. </P>
<Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_5.jpg"/>
Spectral FFT(“Fraunhofer”)• Intensity I0• Scale w0Spectral FrFT(“Fresnel”)• Frac. order α• Num. blades• Blade shape• NoiseBundle planning• Lens prescriptionRay tracing• Lens prescription• AR coatings• Spectral bands• Grid resolutionRasterizer• MSAA?• Spectral filter?Geom. touch-upCompute irradiance• Cull invalid tris?• Use symmetry?Starburst texturePolygon textureBundle boundsDetail levelRay gridRel. intensitiesClipping coordsPrimitivesGhost texturePrecomputationFor each ghostAperture generator• Mixing weights• Tone mappingFramebuffer</Figure>

<P>Figure 7: Our lens fare rendering pipeline. </P>

<P>Assumption We will assume a directional, or distant, light source which holds for most sources of fare (e.g., sunlight, street lights, and car headlamps). This assumption is not a necessary requirement of our algorithm, but helpful for its acceleration. </P>
</Sect>

<Sect>
<Sect>
<H5>4.1 Rendering Scheme </H5>

<P>Ghost Enumeration Rays traversing the lens system are refected or refracted at lenses. Each fare element caused by interrefection, henceforth called “ghost”, corresponds to a specifc sequence of these transmissions and refections. Only sequences involving an even number of refections impinge on the sensor. Those with more than two refections can usually be ignored; only a small percentage of light is refected and they are typically by orders of magnitude weakened leading to insignifcant contributions in the 
<Link>fnal image (Figure 8). We enumerate all two-refection sequences: </Link>
light enters the lens barrel, propagates towards the sensor, is refected </P>
</Sect>
<Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_6.jpg"/>
Figure8:Ghosts caused by 2-fold (left) and 4-fold (right) inter-
<Caption>
<P>Figure 10: Pushing the limits of a biconvex lens (illustration). Left: traditional intersection with the nearest surface along the way. Right: our intersection in fxed sequence. Note how the outermost rays still converge to the focus. </P>
</Caption>
</Figure>

<Sect>
<P>refection. The small percentage refected at each surface signifcantly weakens higher-order fare, despite the higher number of such ghosts. To improve performance, we do not render them by default. </P>
<Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_7.jpg"/>
</Figure>

<P>Figure 9: One out of 91 possible double-refection sequences for this lens design </P>

<P>at an optical surface, travels back, is again refected, and, fnally, reaches the sensor 
<Link>(Figure 9). For </Link>
n Fresnel interfaces in an optical system, there are N = n(n − 1)/2 such sequences that are treated independently to render one ghost at a time. </P>

<P>Bundle Tracing For a given ghost index and incident light direction, a parallel bundle of rays is spanned by the entrance aperture of the lens barrel. Next, we select a sparse uniform set of rays to track through the lens system. Because we know the exact intersection sequence for each ghost, unlike classical ray tracing, we do not need to follow each ray with a recursive scheme, elaborate intersection tests, or spatial acceleration structures. We parse the sequence into a deterministic order of intersection tests against the algebraically-defned lens surfaces. This makes our technique particularly well suited for GPU execution. </P>

<P>At each intersection, we compare the hitpoint of the ray with the diameter of the respective surface and record its maximum normalized distance from the optical axis along the way through the system: </P>

<P>(new) (old)</P>

<P>r = max(r , r/rsurface),</P>

<P>rel rel </P>

<P>where r is the distance of the hitpoint to the optical axis, and rsurface the radius of the optical element. Also, as a ray passes through the aperture plane, a pair of intersection coordinates (ua,va) is stored. Note that we do not discard rays that escape from the system (rrel > 1), since even these are valuable for interpolation in the ray grid (see below). Furthermore, we extrapolate the functionality of each optical surface virtually by relying on its defning algebraic function beyond the nominal lens diameter. To make this extension work, we replace the common nearest-surface check with a strict in-order intersection, all the while allowing the resulting ray parameter to be negative 
<Link>(Figure 10). </Link>
This increases the numerical stability of the simulation for small ray densities, since more rays can pass through the system in a mathematically continuous way. Note that these non-physical rays will not end up in the fnal rendering but only serve as data points for interpolation. For interpolated rays that are actually drawn, the next surface is also the nearest. </P>

<P>Only when a ray can no longer be intersected with the next surface at all, or undergoes total internal refection, it is pruned. This can create holes in the ray grid, but we did not see the need for any refnement strategies. It simply proved unproblematic because the ray’s transported energy approaches zero in the vicinity of total inner refection, making its neighbors and the area on the ray grid appear black in the fnal rendering anyway. </P>

<P>Rasterization and Shading Once the rays have been traced through the system, they form a ray grid on the sensor plane (Fig
<Link>ure 11). The set of rays is sparse and, each ray taken by itself, would </Link>
deliver insuffcient quality. Our goal is to interpolate information from neighboring rays to estimate the behavior of an entire ray beam. To this end, we do not use a random sparse set of rays, but initialize the ray set as a uniform grid placed at the frst lens element. Each grid cell on the entrance plane can be matched to a grid cell on the sensor between the same rays. Similarly to traditional beam tracing, the total radiant power transported through each beam is now distributed evenly over the area of the corresponding quad, leading to intensity variations in the lens fare. Additional shading terms (in particular, Lambertian cosine terms) are taken into account. </P>
</Sect>

<P>Note that so far, we did not cull rays that were blocked by the lens system or aperture, but we recorded the position where they traversed the aperture (ua,va), and its maximum distance to the optical axis, rrel, with respect to the radius of the respective surface. When treating a beam, we can now interpolate these coordinates over the corresponding quad. Hereby, more accurate inside/outside checks for the interpolated rays become possible; we apply clipping on a fragment basis when the interpolated radius exceeds the limit distance. Finally, the position on the aperture is used to decide the ghost shape by a lookup in an aperture texture. This is also when the “Fresnel-like” diffraction 
<Link>comes in (Section 3.3), since the ringing </Link>
pattern has been precomputed and stored in the aperture texture. </P>
</Sect>

<Sect>
<Sect>
<H5>4.2 Accelerations </H5>

<P>The previously described algorithm delivers convincing results for simple lens systems. Here, we present several strategies to improve upon the basic solution in terms of quality and speed. </P>

<P>Ray Bounding Of all the rays entering the lens from a given direction, only a small subset can actually reach the sensor. Many rays are blocked by obstacles, in particular, the iris aperture when it is set to a small diameter. To save computational resources, we therefore restrict the sparse set of rays to a rectangular region on the entrance aperture that is chosen to enclose all rays that might potentially make it all the way to the sensor. </P>

<P>The location and dimensions of this bounding region depend on the light direction, aperture size, and possibly other parameters (zoom, or focus) in a nontrivial way, making a run-time parameter search diffcult. Instead, we propose a preprocessing step to estimate the optimal bounding region for each ghost. For a given confguration, we employ the previous basic algorithm with a low resolution grid to recover all rays that actually reach the sensor. Based on this grid, we determine a bounding rectangle on the entrance aperture. It proved </P>
</Sect>
<Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_8.jpg"/>
Figure13:Mirror symmetry -only the iris shape causes asymmetry 
<Caption>
<P>Figure 11: From left to right: A ray bundle mapped to a grid on the sensor plane (color-coded aperture texture coordinates (ua,va), clipping radius rrel, shading with aperture texture and clipping). </P>
</Caption>
</Figure>

<Sect><Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_9.jpg"/>
</Figure>

<P>Figure 12: A highly complex ghost. From left to right: Deformed ray grid on sensor, aperture texture coordinate, rendered caustic. </P>

<P>suffcient to make the bounds wide enough to contain all valid rays for the current and all neighboring parameter settings. A subdivision scheme can help in speeding up the bounding procedure. </P>

<P>Adaptive Resolution Lens fare is a set of caustics of a complex optical system, which also implies that very high frequencies can occur. While ray bounding signifcantly improves performance and quality, subtle changes might still be missed. In our algorithm, a regular grid of incident rays is mapped to a more or less homogeneous grid on the sensor. In most cases, the grid undergoes simple scaling and translation which is captured with suffcient precision even for a coarse 
<Link>tessellation (Figure 11). </Link>
In some confgurations, though, the accumulation of nonlinear effects can cause severe deformations, fold the grid onto itself, or even 
<Link>change its topology (Figure 12). </Link>
Such ghosts require a higher grid resolution. </P>

<P>We employ a heuristic approach to adapt the grid resolution for each ghost. As an indicator, we use the area of grid cells. A large variance across the grid implies that a non-uniform deformation occurred and higher precision is needed. We therefore evaluate this variance during precomputation, and assign one out of six detail levels to each ghost, with resolutions from 16 × 16 to 512 × 512 rays per bundle. Thus, through early identifcation of challenging ghosts the use of more sophisticated subdivision techniques can be avoided. </P>

<P>Intensity LOD Another piece of information obtained during the precomputation step is an approximate intensity of the resulting ghost. Given this information, during runtime, the user can control the budget by fxing the number of brightest ghosts to be evaluated. </P>

<P>Aperture Culling For small iris openings, rays rarely traverse the aperture multiple times without being blocked. As a result, the corresponding two-refection sequences (with three aperture traversals) can usually be ignored without introducing strong artifacts. Hereby, the number of enumerated sequences is reduced signifcantly to N =(f(f − 1) + b(b − 1))/2, where f and b are the number of lens surfaces in front of or behind the aperture, respectively. </P>

<P>Symmetries Symmetries in the optical system can help reduce computational complexity. By design, most photographic lenses are axisymmetric, whereas anamorphic lenses (featuring two orthogonal planes of symmetry that intersect along the optical axis) are common in the flm industry. The latter are currently not supported in our system, but they could be added by replacing the spherical lens surfaces with more general, ellipsoidal, shapes. </P>

<P>For axial symmetry, we can reduce the amount of required precomputation drastically; all computation up to and including the ray tracing is done for a fxed azimuthal angle of incidence, and then rotated into place. Furthermore, we can reduce the sparse ray set by exploiting the mirror symmetry of the fare arrangement, only considering half the rays on the entrance plane. The grid on the sensor can then be mirrored along the symmetry axis. Please notice that our choice to not block rays directly, but record aperture coordinates and intersection distances, enables us to consider the whole system as symmetric—even the aperture, which in general is not, just as the resulting ghosts are not symmetric. </P>

<P>Spectral Rendering Treating antirefective coating and chromatic lens aberrations requires a wavelength-dependent evaluation. For a brute-force evaluation, most ghosts are well represented with only three wavelengths (RGB), but a few (typically, 3 out of 140 ghosts), can require up to 60 wavelengths for smooth results. While a level-ofdetail (LOD) approach could be imagined, we render at 3 (standard quality / RGB) or a maximum of 7 (high quality) wavelengths, which comes at a moderate computational cost, but employ an interpolation strategy. We flter the result of each wavelength band in image space to create transitions. The orientation and dimension of the required 1D blur kernel have been derived during the precomputation phase, from the spatial variation between neighboring wavelength bands. The flter size is chosen to bridge the gap between the bands. The fltered representations are then blended together in the RGBA framebuffer and deliver a smooth result. </P>
</Sect>
</Sect>

<Sect>
<Sect>
<H5>4.3 GPU Implementation </H5>

<P>Basic Algorithm We perform the ray tracing in the vertex shader. To deal with total refection, culled rays are fagged via a texture coordinate. The geometry shader then produces the triangle strips that form beam quads in the grid. Here, the area of each grid cell is also computed, and used to compute the irradiance. For symmetric systems where only half of the ray set is traced, the geometry shader mirrors each triangle along the symmetry axis of the fare arrangement. This doubling of triangles is more effcient than image-based mirroring. The resulting quads on the sensor are rasterized in the fragment shader that can discard fragments if they correspond to blocked rays (rrel > 1, see 
<Link>Section 4.1). </Link>
Per-vertex irradiance values are interpolated over the quad, and a texture lookup based on the aperture coordinate completes the rendering. Finally, all ghosts are composited additively. </P>
</Sect>
<Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_10.jpg"/>

<Caption>
<P>Figure 14: SIGGRAPH logo aperture shape (left) and a procedural “dirt” pattern consisting of dots and lines (right), each along with its chromatic Fourier transform. </P>
</Caption>
</Figure>
<Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_11.jpg"/>
Canon EF 70–200mm f/2.8LNikon 80–200mm f/2.8Itoh 100–145mm f/3.5Angenieux Biotar 100mm f/1.1Kreitzer Tele 390mm f/5.6Brendel Tessar 100mm f/2.80.5/7.5 fps0.7/9.5 fps1.2/23 fps3.2/30 fps3.1/39 fps8.6/47 fps1.8/58 fps6.4/84 fps7.7/110 fps29/110 fps18/228 fps21/189 fps
<Caption>
<P>Figure 15: Lens fare of various lens systems. Fps are given for high quality (more rays do not bring improvement) and standard settings. Top row: lens layout. Middle row: aperture fully open. Bottom row: aperture reduced by 4 f-stops. </P>
</Caption>
</Figure>

<Sect>
<P>Smooth shading An improvement in quality can be achieved by not shading quads, but vertices and interpolating (Gouraud shading). At each vertex, we store the average value of its surrounding neighbors. The regular grid of rays, combined with the transform feedback (or the stream-out) mechanism of modern graphics hardware, makes this lookup of neighboring quad values very easy. </P>
</Sect>
</Sect>
</Sect>

<Sect>
<Sect>
<H5>5 Artistic Control </H5>

<P>Like 
<Link>other lens effects [Lee et al. 2010], fare </Link>
can serve as a creative tool to increase the appeal of synthetic images and photographs alike. Our algorithm offers many possibilities to interact with the basic pipeline in order to exceed physical limitations while maintaining a plausible look. Due to our approach of enumerating all interrefections, only certain, maybe the most beautiful, ghosts can be selected for rendering. Furthermore, imperfections can be well represented with very approximate means. </P>

<P>Creative Use of Optical Elements While the most common lenses feature apertures shaped like regular polygons, any 2D shape can be used instead. As an example, we used the SIGGRAPH logo which results in an 
<Link>unusual starburst pattern (Figures 14) </Link>
as well as 
<Link>transformed ghosts of the logo all over the image (Figures 16). </Link>
</P>

<P>Adding Realism through Imperfections Lenses in the real world are often degraded by dust and imperfections on the surface that can affect the diffraction pattern. We give control over this effect by adding a texture of dust and scratches to the aperture before determining the Fourier spectrum. Drawing a dirt texture is possible, but we also offer procedural generation of scratches and dust based on user defned statistics (density, orientation, length, size). While scratches add new streaks to the lens fare, dust has a tendency to add rainbow-colored speckles. In addition, the texture could be animated to achieve dynamic effects as 
<Link>in [Ritschel et al. 2009]. </Link>
</P>
<Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_12.jpg"/>
Video with added flareStylized lens flareOriginal video
<Caption>
<P>Figure 16: HDR video frames with added post-process lens fare. Left: “SIGGRAPH” lens equipped with a custom aperture shape. Right: A HDR frame from the short “Fiat Lux” by Paul Debevec, seen through the Nikon lens. </P>
</Caption>
</Figure>

<P>Imperfect Symmetries Since real lens systems are never exactly symmetric, lens fare does not line up perfectly on the mirror axis. To model this imperfection, we add a variance that translates each ghost slightly in the image plane. This modifcation offers more intuitive control than a corresponding change in the lens system. </P>

<P>Anti-Refective Coating The color of each individual ghost is mainly determined by the anti-refective coatings of the lens surfaces causing it. This effect can easily be abstracted by letting the user provide color ramps or global color changes for each ghost. </P>
</Sect>
</Sect>

<Sect>
<Sect>
<H5>6 Results </H5>
</Sect>

<P>We implemented our solution on an Intel Core 2 Quad 2.83 GHz with an NVIDIA GTX 285 card. Our method reaches interactive to real-time framerates depending on the complexity of the optical sys
<Link>tem and the accuracy of the simulation. As illustrated in Figure 15, </Link>
our method can be of interest for demanding real-time applications, 
<Link>but also for higher-quality simulations. Figure 19 shows </Link>
that even at signifcantly reduced resolution, the ghosting computed using our technique is close to the ray-traced reference. For performance, one could even pick only those ghosts that are particularly beautiful, yielding a signifcant speedup while maintaining the artistic expression. In practice, culling the 20% weakest ghosts delivers 20% speedup without introducing visible artifacts. Even 40% still proved acceptable for interactive applications (speedup approx. 50%). In 
<Link>Figure 15, </Link>
we provide performance ratings for different quality settings. A side-by-side comparison of the settings used can be found 
<Link>in Figure 17. </Link>
The most costly effect of our solution are caustics in highly anisotropic 
<Link>ghosts (e.g., Figure 18) because here the ray </Link>
bundles are most sensitive to spectral and spatial variation. </P>

<Sect>
<P>Our solution does need to perform a reasonably quick precomputation step to bound the sparse set of rays. For a simple lens such as a Brendel prime lens (9 ghosts), this precomputation takes less than </P>

<P>0.1 sec; for the Nikon zoom lens (142 ghosts), it takes 5 min.; for the Canon zoom lens (312 ghosts), it takes 20 min to iterate through all ghosts × 90 light directions × 642 rays × 20 zoom factors × 8 aperture stops. The latter two allow us to freely change camera settings on the fy. </P>

<P>Our algorithm produces physically-plausible lens fare renderings 
<Link>(Figure 1). Most important effects </Link>
are simulated convincingly, leading to images that are hard to distinguish from real-world footage. </P>
<Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_13.jpg"/>

<Caption>
<P>Figure 17: Quality settings. Left: high quality (7 spectral bands, spectral fltering, supersampling). Right: normal quality (RGB, no fltering, no supersampling, remove 40% darkest ghosts). The corresponding frame rates are 6.1 fps and 20.6 fps, respectively, on an NVIDIA GTX 285. </P>
</Caption>
</Figure>
<Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_14.jpg"/>

<Caption>
<P>Figure 18: Five different views on “Ghost #103” in the Nikon lens system. Note the intricate folding and chromatic variation. </P>
</Caption>
</Figure>

<P>The main difference arises from imperfections of the lens system and our approximate handling of diffraction effects. Furthermore, the lens coating is unknown, forcing us to an estimate. </P>

<P>Our algorithm naturally handles complex deformations and caustics 
<Link>(Figure 18). Previous </Link>
real-time methods were unable to obtain similar results because this effect can only be reproduced when light paths through the system are simulated. Our model considers many aspects that were neglected by previous approaches (e.g., the refectivity of lens coatings as a function of wavelength and angle). Even with these improvements and at highest spatial and spectral resolutions, rendering fares for even the most complex optical designs takes no more than a few seconds. This is signifcantly faster than a typical path-traced solution that would take hours, if not days, to converge on today’s 
<Link>desktop computers [Steinert et al. 2011]. </Link>
</P>

<P>The memory consumption of our algorithm is mainly defned by the textures containing the aperture and its Fourier transform (24 MB worth of 16-bit foat data), as well as three render buffers (another 24 MB). </P>
</Sect>
</Sect>

<Sect>
<Sect>
<H5>7 Discussion and Limitations </H5>
</Sect>

<Sect>
<Sect>
<H5>7.1 Applications </H5>

<P>Our algorithm has low memory overhead and is computationally effcient. It could be of use for various application scenarios: </P>
<Figure>

<ImageData src="images/Physically-Based Real-Time Lens Flare Rendering_compressed_img_15.jpg"/>
</Figure>
</Sect>

<P>Figure 19: Left: densely ray-traced rendering (resolution per ghost 81922 samples; rendering time 159 s on an NVIDIA GTX 580). Middle: rendering using sparse ray bundles at a maximum resolution of 1282 , and our interpolation technique (29.8 ms per frame on the same hardware). Right: A closer look on one of the ghosts: underlying grid structure and aperture coordinates. </P>

<Sect>
<P>CG Movie Production The quality delivered by our rendering scheme exceeds many previous offine techniques, making it interesting as a preview, but even as a fnal rendering solution. Artistic control further allows a user to maintain a realistic appearance while being able to fne-tune and customize the effect. </P>

<P>Computer Games Deactivating costly calculations increases the overall performance, making our solution useful for games. Furthermore, our two-refection assumption allows the user to choose particular ghosts that they consider important. For well-behaved fares, even a very small amount of rays (e.g., 4 × 4) delivers high quality thanks to the interpolation step. </P>

<P>Image and Video Processing Current lens fare flters do not appear convincing because they keep a static look, e.g., ghost deformations are ignored. Our method is temporally coherent, making it a good choice for movie footage as well. We detect and follow light sources in the image (using an intensity threshold). One could also animate the light manually to emphasize scene elements or guide the observer. Our instant feedback is of great help in this context. </P>

<P>Lens-System Design Even in live-action cinematography, lens fare is sometimes considered a 
<Link>desired effect [Woerner 2009]. Cer</Link>
tain lens systems (such as the Lensbaby) are designed for creative use of aberrations and other optical effects. In combination with a traditional lens design tool, our algorithm could help lens designers to preview and optimize the fare characteristic of an optical design. </P>

<P>Defaring Another interesting possibility would be to predict and remove fare patterns from actual photographs. However, this would require a perfect description of the optical system with its countless parameters und unknowns, which is currently out of reach. </P>
</Sect>
</Sect>

<Sect>
<H5>7.2 Limitations </H5>

<P>Light Sources Our current rendering mechanism is optimized to having a single visible point/directional light source. Area lights can be approximated by a point light and an energy emission proportional to their visibility, but such solutions remain approximate. A more accurate possibility is to sample the source at an additional cost. </P>

<P>Precomputing Resolution While the ray bounding precomputation traces rays through the system for various light positions, one should not conclude that these low-resolution images could be used for rendering. We initially experimented with image-warping strategies, but it proved futile because the subtle changes (such as displacement and deformation) cannot be well addressed. Also, such a solution is memory intensive, while ours only stores a small lookup table of bounding rectangles. </P>

<P>Aliasing A common problem when triangles become smaller than one pixel is rasterization aliasing. The situation can lead to very high intensity, but potentially error-prone rasterization. In practice, this only happens for very anisotropic ghosts (min/max ratio > 1000) and their number is very small (for the Nikon lens, 3 out of 142). If desired, we can select these fares and treat them with a higher resolution framebuffer which is, in the end, added to the standard framebuffer. Alternatively, one can replace the rasterization of these small triangles with a point rendering technique. </P>
</Sect>
</Sect>

<Sect>
<H5>8 Conclusion </H5>

<P>We presented an interactive rendering algorithm to simulate lens fare of complex lens systems. We showed superior results with respect to previous interactive solutions and even offine suggestions to a large extent. We also introduced various means to artistically modify and enhance the rendition beyond physical limitations for stylization purposes. Our algorithm is fexible in the sense that we allow a fne tradeoff between accuracy and performance by allowing the user to choose the simulated effects that are most important for the application context. Hereby, our method addresses high-quality, as well as medium-quality real-time purposes. </P>

<P>In the future, we imagine that our fast algorithm could serve as a tool to recover parameters of unknown optical systems. Given the fact that fare patterns are very sensitive to slight parameter changes, an analysis-by-synthesis scheme could enable nondestructive lens characterization. Such a calibrated fare synthesis might then enable us to “defare” HDR images taken by the system. Capturing artifact-free HDR data is an unsolved problem. </P>

<P>We thank the anonymous reviewers for their valuable comments and suggestions. This work was partly funded by the Intel Visual Computing Institute at Saarland University. Sungkil Lee gratefully acknowledges support by the Basic Science Research Program through the National Research Foundation of Korea, funded by the Ministry of Education, Science and Technology (2011-0014015). </P>
</Sect>
</Sect>

<Sect>
<H5>References </H5>

<P>ALSPACH, T., 2009. Vector-based representation of a lens fare. US Patent 7,526,417. </P>

<P>CHAUMOND, J., 2007. Realistic camera -lens fares. </P>

<P>
<Link>http://graphics.stanford.edu/wikis/ </Link>
</P>

<P>
<Link>cs348b-07/JulienChaumond/FinalProject. </Link>
</P>

<P>ERNST, M., AKENINE-M ¨</P>

<P>OLLER, T., AND JENSEN, H. W. 2005. Interactive rendering of caustics using interpolated warped volumes. In Proc. Graphics Interface’05, 87–96. </P>

<P>GOODMAN, J. W. 2005. Introduction to Fourier Optics, 3 ed. Roberts &amp; Company Publishers, December. </P>

<P>HECHT, E. 2001. Optics, 4 ed. Addison Wesley, August. </P>

<P>KESHMIRIAN, A. 2008. A physically-based approach for lens fare simulation. Master’s thesis, University of California, San Diego. </P>

<P>KILGARD, J., 2000. Fast OpenGL-rendering of lens fares. </P>

<P>
<Link>http://www.opengl.org/resources/features/ </Link>

<Link>KilgardTechniques/LensFlare/. </Link>
</P>

<P>KING, Y. 2001. Game Programming Gems 2. Charles River Media, ch. 2D Lens Flare. </P>

<P>KINGSLAKE, R. 1992. Optics in Photography. SPIE Publications. </P>

<P>KOLB, C., MITCHELL, D., AND HANRAHAN, P. 1995. A realistic camera model for computer graphics. In Proc. ACM SIGGRAPH, 317–324. </P>

<P>LEE, S., EISEMANN, E., AND SEIDEL, H.-P. 2010. Real-Time Lens Blur Effects and Focus Control. ACM Transactions on Graphics (Proc. ACM SIGGRAPH’10) 29, 4, 65:1–7. </P>

<P>MAUGHAN, C. 2001. Game Programming Gems 2. Charles River Media, ch. Texture Masking for Faster Lens Flare. </P>

<P>OAT, C. 2004. Shader X3. Charles River Media, ch. A Steerable Streak Filter. </P>

<P>OGAWA, H., 1996. Zoom lens. US Patent 5,537,259. </P>

<P>OH, S. B., KASHYAP, S., GARG, R., CHANDRAN, S., AND RASKAR, R. 2010. Rendering Wave Effects with Augmented Light Field. In Computer Graphics Forum (Proc. Eurographics). </P>

<P>OZAKTAS, H. M., ZALEVSKY, Z., AND KUTAY, M. A. 2001. </P>

<P>The fractional Fourier transform with applications in optics and signal processing. Wiley. </P>

<P>PERRIN, J.-C. 2004. Methods for rapid evaluation of the stray light in optical systems. SPIE, L. Mazuray, P. J. Rogers, and </P>

<P>R. Wartmann, Eds., vol. 5249, 392–399. </P>

<P>PIXAR, 2008. The imperfect lens: Creating the look of Wall-E. Wall-E Three-DVD Box. </P>

<P>POLYANSKIY, M., 2010. Refractive index database. 
<Link>http:// </Link>

<Link>refractiveindex.info. </Link>
</P>

<P>RITSCHEL, T., IHRKE, M., FRISVAD, J. R., COPPENS, J., MYSZKOWSKI, K., AND SEIDEL, H.-P. 2009. Temporal Glare: Real-Time Dynamic Simulation of the Scattering in the Human Eye. In Computer Graphics Forum (Proc. Eurographics). </P>

<P>SCHOTT AG, 2011. Optical glass catalogue, January 2011. </P>

<P>SEKULIC, D. 2004. GPU Gems. Addison-Wesley, ch. Effcient Occlusion Queries. </P>

<P>SELLMEIER, W. 1871. Zur Erkl¨</P>

<P>arung der abnormen Farbenfolge im Spectrum einiger Substanzen. Annalen der Physik und Chemie 219, 272–282. </P>

<P>SMITH, W. J. 2005. Modern Lens Design. McGraw-Hill. </P>

<P>STEINERT, B., DAMMERTZ, H., HANIKA, J., AND LENSCH, H. </P>

<P>P. A. 2011. General spectral camera lens simulation. In Computer Graphics Forum, vol. 30, to appear. </P>

<P>TOCCI, M., 2007. Quantifying Veiling Glare (ZEMAX Users’ Knowledge Base). 
<Link>http://www.zemax.com/kb/ </Link>

<Link>articles/192/1. </Link>
</P>

<P>WENZEL, C., 2005. Far Cry and DirectX. 
<Link>http: </Link>

<Link>//developer.amd.com/media/gpu_assets/ </Link>

<Link>D3DTutorial08_FarCryAndDX9.pdf. </Link>
</P>

<P>WOERNER, M., 2009. J.J. Abrams Admits Star Trek Lens Flares Are “Ridiculous” (interview). 
<Link>http://io9.com/#!5230278. </Link>
</P>
</Sect>
</Part>
</TaggedPDF-doc>
